<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <description>Comics + Machine Learning = Awesomeness</description>
    <link>http://127.0.0.1:4000/</link>
    <atom:link href="http://127.0.0.1:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sat, 24 Sep 2016 00:55:59 -0400</pubDate>
    <lastBuildDate>Sat, 24 Sep 2016 00:55:59 -0400</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>Lstm</title>
        <description>&lt;p&gt;LSTM is a type of Recurrent Neural Network. So in order to understand what a LSTM is, we must understand the basic structure of a Recurrent Neural Network(RNN).&lt;/p&gt;

&lt;h4 id=&quot;what-is-rnn&quot;&gt;What is RNN?&lt;/h4&gt;
&lt;p&gt;If you are familiar with Neural Networks, then you can imagine RNN as a recurrsive Netural Networks. Recurrent Neural Network takes in the output of the past classifiers and a new input to make a prediction for the new input
&lt;!-- shared parameter to extract pattern over time --&gt;&lt;/p&gt;

&lt;p&gt;![alt text][RNNgraph]&lt;/p&gt;

&lt;p&gt;Even tho Recurrent is often referred as a Recursive Neural Network, it is more like a recursive hidden cell. The RNN cell applies a matrix, which is referred as the weights of the Neural Network, to every single inpus that it reads in. It performs a matrix multiplication to the input to produce the matrix of predictions.&lt;/p&gt;

&lt;p&gt;![alt text][RNNsimplified]&lt;/p&gt;

&lt;p&gt;In other words, you can consider the Recurrent Neural Network as a cell that performs some operation on each instance and incorporates previous outputs to predict the output.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Getting Technical&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We can also represent RNN as a Feedforward Neural Network. The main difference between a Normal Feedforward Neural Network and a RNN is that some part of the inputs will be passed to the middle of the Neural Network instead of the start. However, there will only be one type of cell that processes the inputs and the parameters will be shared across the Neural Network.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;I will write another blog that will explain the details of RNN&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;what-is-lstm&quot;&gt;What is LSTM?&lt;/h4&gt;
&lt;p&gt;LSTM is very similar to the vanilla RNN, the main difference between them is the hidden layers. LSTM has additional matrixes which are refered as “gates”. These matrices are responsible to eliminating instances that are not 
Each LSTM cell is consist of input, a memory cell, and output.&lt;/p&gt;

&lt;h4 id=&quot;what-are-memory-cells&quot;&gt;What are Memory cells?&lt;/h4&gt;
&lt;p&gt;A memroy cell has write, read, and forget gate. This is often referred as the three gates for LSTM. They are called gates because their functionality is the control the flow of the incomers(or input) and filter those that do not qualify.&lt;/p&gt;

&lt;p&gt;![alt text][LSTM Memory cell]&lt;/p&gt;

&lt;p&gt;The way each gates control the flow of the input is by using a Sigmoid function. The output of the sigmoid function(also often referred as a sigmoid layer) is between 0 to 1.
&lt;!--sigmoid graph with mathematical equation on top, the graph will explain the difference between 0 and 1, everything in between gets partially reserved--&gt;
![alt text][sigmoidgraph]&lt;/p&gt;

&lt;p&gt;This sigmoid function allows the model to filter unwanted data through a mathematical way, which is much more efficient.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Getting Technical&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Vanishing gradient problem and Exponential gradient problem is a common issue for Feedforward Neural Networks. We can solve the exponential gradient problem by putting a cap on the gradient. However, the Vanishing Gradient Problem is slightly harder to solve. The three gates of LSTM help solves the vanishing gradient problem.&lt;/p&gt;

&lt;p&gt;Since multiplicative factor for each gate is a sigmoid function and sigmoid function is continuous everywhere, this means the function is differentiable. One of the benefits of having a differential multiplicaive factor means that we will be able to perform Back Propergation on the algorithm. Back Propergation will provide us a computationally inexpensive way to calculate the derivative of loss function, which will be necessary for tweaking the weights matrix.&lt;/p&gt;

&lt;p&gt;LSTM also applies a Tanh function before senting its prediction to the output gate. The tanh function keeps output between -1 to 1 and transforms the output to reduce the effect of outlying values.&lt;/p&gt;

&lt;!--Analogy: A amusement park--&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;what-are-the-applications-for-lstm&quot;&gt;What are the applications for LSTM&lt;/h4&gt;

&lt;p&gt;LSTM is especially successful in many NLP tasks and analying time series data, or seuqencial data. The property of LSTM allows it to capture patterns with long-term dependencies better than vanilla RNNs. It is usually used for sentence generation, word sentiment understanding, or sentence sentiment analysis.&lt;/p&gt;

&lt;!-- Machine Learning Terms Cheat Sheet --&gt;

&lt;p&gt;[RNNgraph]
[RNNsimplified]
[sigmoidgraph]
[LSTM Memory cell]&lt;/p&gt;
</description>
        <pubDate>Thu, 22 Sep 2016 00:00:00 -0400</pubDate>
        <link>http://127.0.0.1:4000/2016/09/22/LSTM/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/09/22/LSTM/</guid>
        
        
      </item>
    
      <item>
        <title>Whatiscomiclearning</title>
        <description>&lt;p&gt;What is Comic Learning?&lt;/p&gt;

&lt;p&gt;My intention of this blog is to record all insteresting NLP techniques and Machine Learning algorithms. I am currently working on a NLP research project with a professor in my university. I wanted to share the exciting knowledge that I am learning and turn the abstract explainations for Machine Learning into comics or drawings that everyone will be able to understand. This blog is aimed for both people who are genuinely curious about the Machine Learning algorithms and those who are advanced in the field and looking for more perspectives.&lt;/p&gt;

&lt;p&gt;I hope my comics will make you giggle and ponder occasionally. If you are new to Artificial Intelligence, I hope this blog will incite your passion in Machine Learning just like the how I was watching my first ML lecture.&lt;/p&gt;

&lt;p&gt;Great thanks to Andrew Ng’s Cousera course and Google’s Deep Learning on Udacity which introduced me to the field.&lt;/p&gt;

&lt;p&gt;And thanks for XKCD for inspiring my idea for “comicfying” ML concepts
 &lt;img src=&quot;https://github.com/rolinawu/rolinawu.github.io/blob/master/_images/xkcdonML.jpg&quot; alt=&quot;alt text&quot; title=&quot;XKCD's comic/comment on Machine Learning&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 05 Sep 2016 00:00:00 -0400</pubDate>
        <link>http://127.0.0.1:4000/2016/09/05/WhatisComicLearning/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/09/05/WhatisComicLearning/</guid>
        
        
      </item>
    
  </channel>
</rss>
